# Nexus Development Environment - Docker Compose
# Complete stack: PostgreSQL + Nexus + LangGraph + MCP + Frontend
#
# Usage:
#   ./docker-demo.sh                    # Start with local environment
#   ./docker-demo.sh --env=production   # Start with production environment
#
# Or directly with docker compose:
#   docker compose -f docker-compose.demo.yml up          # Start all services
#   docker compose -f docker-compose.demo.yml up -d       # Start in background
#   docker compose -f docker-compose.demo.yml down        # Stop all services
#   docker compose -f docker-compose.demo.yml logs -f     # View logs
#
# Environment Files (loaded by docker-demo.sh):
#   Local mode:      .env (or .env.example as fallback)
#   Production mode: .env.production + .env.production.secrets
#
# Services:
#   - postgres:    PostgreSQL database (port 5432)
#   - nexus:       Nexus RPC server (port 2026)
#   - mcp-server:  Nexus MCP server (port 8081)
#   - langgraph:   LangGraph agent server (port 2024)
#   - frontend:    React web UI (port 5173)

version: '3.8'

services:
  # ============================================
  # PostgreSQL Database (PostgreSQL 18)
  # Features: Async I/O, Skip Scan, UUIDv7, OLD/NEW RETURNING
  # ============================================
  postgres:
    image: postgres:18-alpine
    container_name: nexus-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-nexus}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-nexus}
    volumes:
      # PostgreSQL 18 default PGDATA: /var/lib/postgresql/data
      - postgres-data:/var/lib/postgresql/data
      - ./postgres-init/01-pg-stat-statements.sql:/docker-entrypoint-initdb.d/01-pg-stat-statements.sql:ro
    # PostgreSQL 18 optimizations:
    # - io_method=worker: Async I/O for 2-3x I/O performance
    # - effective_io_concurrency=16: Concurrent I/O requests (tuned for SSD)
    # - enable_self_join_elimination=on: Query optimizer improvement
    # - wal_compression=lz4: 30-50% WAL size reduction with fast compression
    # pgvector HNSW tuning (Issue #1004):
    # - maintenance_work_mem=2GB: Faster index builds for 100K+ vectors
    # - max_parallel_maintenance_workers=7: Parallel index build (up to 30x faster)
    # - shared_buffers=1GB: Cache HNSW index in memory
    # See: docs/performance/vector-search-tuning.md
    command: >
      postgres
      -c shared_preload_libraries=pg_stat_statements
      -c pg_stat_statements.track=all
      -c io_method=worker
      -c effective_io_concurrency=16
      -c maintenance_io_concurrency=16
      -c enable_self_join_elimination=on
      -c enable_distinct_reordering=on
      -c wal_compression=lz4
      -c maintenance_work_mem=2GB
      -c max_parallel_maintenance_workers=7
      -c shared_buffers=1GB
      -c effective_cache_size=3GB
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-nexus}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - nexus-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Nexus RPC Server
  # ============================================
  nexus:
    build:
      context: .
      dockerfile: Dockerfile
    image: nexus-server:latest
    container_name: nexus-server
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      dragonfly:
        condition: service_healthy
    environment:
      # Server configuration
      NEXUS_HOST: 0.0.0.0
      NEXUS_PORT: 2026
      NEXUS_DATA_DIR: /app/data

      # Database configuration (PostgreSQL)
      NEXUS_DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-nexus}@${POSTGRES_HOST:-postgres}:${POSTGRES_PORT:-5432}/${POSTGRES_DB:-nexus}
      # TokenManager database (for OAuth credentials)
      TOKEN_MANAGER_DB: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-nexus}@${POSTGRES_HOST:-postgres}:${POSTGRES_PORT:-5432}/${POSTGRES_DB:-nexus}
      # Increase pool size for provisioning with many files
      NEXUS_DB_POOL_SIZE: ${NEXUS_DB_POOL_SIZE:-20}
      NEXUS_DB_MAX_OVERFLOW: ${NEXUS_DB_MAX_OVERFLOW:-30}
      NEXUS_DB_POOL_TIMEOUT: ${NEXUS_DB_POOL_TIMEOUT:-30}
      # Skip heavy skills in CI to avoid connection pool exhaustion
      NEXUS_SKIP_HEAVY_SKILLS: ${NEXUS_SKIP_HEAVY_SKILLS:-false}

      # Authentication
      # Admin user for initialization (default: admin)
      NEXUS_ADMIN_USER: ${NEXUS_ADMIN_USER:-admin}

      # Optional: Pre-set API key (otherwise auto-generated on first run)
      # Leave empty to auto-generate - check logs with: docker logs nexus-server
      NEXUS_API_KEY: ${NEXUS_API_KEY:-}

      # Backend storage (local by default, can be 'gcs' for production)
      NEXUS_BACKEND: ${NEXUS_BACKEND:-local}
      NEXUS_GCS_BUCKET: ${NEXUS_GCS_BUCKET:-}
      NEXUS_GCS_PROJECT: ${NEXUS_GCS_PROJECT:-}

      # GCS credentials (automatically mounted at /app/gcs-credentials.json)
      # Override with environment variable if using a different path
      GOOGLE_APPLICATION_CREDENTIALS: ${GOOGLE_APPLICATION_CREDENTIALS:-/app/gcs-credentials.json}

      # AWS credentials for S3 connector
      AWS_SHARED_CREDENTIALS_FILE: ${AWS_SHARED_CREDENTIALS_FILE:-/app/aws-credentials}
      AWS_CONFIG_FILE: ${AWS_CONFIG_FILE:-/app/aws-config}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}

      # OAuth encryption key (for secure token storage)
      # Generate with: python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
      NEXUS_OAUTH_ENCRYPTION_KEY: ${NEXUS_OAUTH_ENCRYPTION_KEY:-}

      # Google OAuth credentials (for Google Drive connector)
      NEXUS_OAUTH_GOOGLE_CLIENT_ID: ${NEXUS_OAUTH_GOOGLE_CLIENT_ID:-}
      NEXUS_OAUTH_GOOGLE_CLIENT_SECRET: ${NEXUS_OAUTH_GOOGLE_CLIENT_SECRET:-}

      # Sandbox configuration
      # URL that sandboxes should use to connect back to Nexus server
      # Use Docker service name for container-to-container communication
      NEXUS_SERVER_URL: http://nexus:2026
      # Docker network for sandboxes to join
      NEXUS_DOCKER_NETWORK: nexus_nexus-network

      # Permission setup (optional)
      # Set to 'true' to skip entity registry and permission setup during init
      NEXUS_SKIP_PERMISSIONS: ${NEXUS_SKIP_PERMISSIONS:-false}

      # Runtime permission enforcement
      # Set to 'false' to disable permission checks at runtime
      NEXUS_ENFORCE_PERMISSIONS: ${NEXUS_ENFORCE_PERMISSIONS:-true}

      # uvloop - disable for debugging compatibility issues
      NEXUS_USE_UVLOOP: ${NEXUS_USE_UVLOOP:-true}

      # Admin bypass (allows admin users to bypass ReBAC checks)
      # Set to 'false' for stricter security in production
      NEXUS_ALLOW_ADMIN_BYPASS: ${NEXUS_ALLOW_ADMIN_BYPASS:-true}

      # Configuration file path (defaults to config.demo.yaml if not specified)
      NEXUS_CONFIG_FILE: ${NEXUS_CONFIG_FILE:-/app/configs/config.demo.yaml}

      # Zoekt code search (optional, requires --profile zoekt)
      # Enable Zoekt for sub-50ms code search on large codebases
      ZOEKT_ENABLED: ${ZOEKT_ENABLED:-false}
      ZOEKT_URL: ${ZOEKT_URL:-http://zoekt:6070}
      ZOEKT_TIMEOUT: ${ZOEKT_TIMEOUT:-10.0}

      # Dragonfly embedding cache (Issue #950)
      # High-performance Redis-compatible cache for embeddings
      # Reduces embedding API calls by ~90%
      NEXUS_DRAGONFLY_URL: ${NEXUS_DRAGONFLY_URL:-redis://dragonfly:6379}
      NEXUS_CACHE_EMBEDDING_TTL: ${NEXUS_CACHE_EMBEDDING_TTL:-86400}
    volumes:
      # Persistent data directory (local backend) - shared with local development
      - ./nexus-data:/app/data
      # Configuration files (includes config.demo.yaml for auto-mounting backends)
      - ./configs:/app/configs:ro
      # GCS credentials (auto-detected - see docker-gcs-setup.sh)
      # Automatically finds credentials from: gcloud, ./gcs-credentials.json, or GCS_CREDENTIALS_PATH
      - ${GCS_CREDENTIALS_PATH:-./gcs-credentials.json}:/app/gcs-credentials.json:ro
      # AWS credentials (auto-detected - see docker-start.sh)
      # Automatically finds credentials from: ~/.aws/credentials or AWS_CREDENTIALS_PATH
      - ${AWS_CREDENTIALS_PATH:-./aws-credentials}:/app/aws-credentials:ro
      - ${AWS_CONFIG_PATH:-./aws-config}:/app/aws-config:ro
      # Docker socket for sandbox provider
      - /var/run/docker.sock:/var/run/docker.sock
    # Run as root to access Docker socket (required for sandbox provider)
    # The nexus user inside the container doesn't have socket permissions
    user: root
    ports:
      - "${NEXUS_PORT:-2026}:2026"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:2026/health"]
      interval: 10s
      timeout: 10s
      retries: 20
      start_period: 120s
    networks:
      - nexus-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Nexus MCP Server
  # ============================================
  mcp-server:
    image: nexus-server:latest
    container_name: nexus-mcp-server
    restart: unless-stopped
    depends_on:
      nexus:
        condition: service_healthy
    entrypoint: []  # Override the docker-entrypoint.sh
    environment:
      # Nexus server URL (internal Docker network)
      NEXUS_URL: http://nexus:2026

      # API Key for authentication (auto-generated by nexus-server)
      NEXUS_API_KEY: ${NEXUS_API_KEY:-}

      # MCP configuration
      MCP_HOST: 0.0.0.0
      MCP_PORT: 8081
    ports:
      - "${MCP_PORT:-8081}:8081"
    command: >
      sh -c '
        echo "ðŸ”„ Waiting for Nexus server to be ready..."
        sleep 5
        echo "ðŸš€ Starting Nexus MCP server on port 8081..."
        echo "   NEXUS_URL: $${NEXUS_URL}"
        exec nexus mcp serve --transport http --port 8081 --host 0.0.0.0
      '
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - nexus-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # LangGraph Agent Server
  # ============================================
  langgraph:
    build:
      context: .
      dockerfile: examples/langgraph/Dockerfile
    image: nexus-langgraph:latest
    container_name: nexus-langgraph
    restart: unless-stopped
    depends_on:
      nexus:
        condition: service_healthy
    environment:
      # LangGraph configuration
      LANGGRAPH_PORT: 2024
      LANGGRAPH_HOST: 0.0.0.0

      # Nexus server URL (internal Docker network)
      NEXUS_SERVER_URL: http://nexus:2026

      # API Keys for LLM providers (required)
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}

      # Optional: Additional tool API keys
      TAVILY_API_KEY: ${TAVILY_API_KEY:-}
      E2B_API_KEY: ${E2B_API_KEY:-}
      FIRECRAWL_API_KEY: ${FIRECRAWL_API_KEY:-}

      # LangSmith tracing (optional)
      LANGCHAIN_TRACING_V2: ${LANGCHAIN_TRACING_V2:-false}
      LANGCHAIN_ENDPOINT: ${LANGCHAIN_ENDPOINT:-}
      LANGCHAIN_PROJECT: ${LANGCHAIN_PROJECT:-}
      LANGSMITH_API_KEY: ${LANGSMITH_API_KEY:-}
    volumes:
      # Persist LangGraph dev history across container rebuilds and restarts
      - ./examples/langgraph/.langgraph_api:/app/.langgraph_api
    ports:
      - "${LANGGRAPH_PORT:-2024}:2024"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:2024/ok"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - nexus-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Frontend (React + Vite)
  # ============================================
  frontend:
    build:
      context: ../nexus-frontend
      dockerfile: Dockerfile
      args:
        # Browser URLs - use localhost for browser to access services
        VITE_NEXUS_API_URL: ${VITE_NEXUS_API_URL:-http://localhost:2026}
        VITE_LANGGRAPH_API_URL: ${VITE_LANGGRAPH_API_URL:-http://localhost:2024}
        # LangGraph server URL - use Docker service name for container-to-container
        VITE_NEXUS_SERVER_URL: ${VITE_NEXUS_SERVER_URL:-http://nexus:2026}
    image: nexus-frontend:latest
    container_name: nexus-frontend
    restart: unless-stopped
    depends_on:
      nexus:
        condition: service_healthy
    environment:
      # Frontend configuration
      VITE_NEXUS_API_URL: ${VITE_NEXUS_API_URL:-http://localhost:2026}
      VITE_LANGGRAPH_API_URL: ${VITE_LANGGRAPH_API_URL:-http://localhost:2024}
    ports:
      - "${FRONTEND_PORT:-5173}:80"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - nexus-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Zoekt Code Search (Optional)
  # Fast trigram-based code search for large codebases
  #
  # Indexes content from:
  #   - LOCAL backend: /data/cas/ (CAS - no L2 cache needed)
  #   - External connectors: /data/.cache/ (L2 disk cache)
  #
  # Usage:
  #   docker compose --profile zoekt up         # Start Zoekt server
  #   docker compose --profile zoekt-index up   # Build/rebuild index
  # ============================================
  zoekt:
    image: sourcegraph/zoekt-webserver:latest
    container_name: nexus-zoekt
    restart: unless-stopped
    profiles:
      - zoekt  # Only starts with: docker compose --profile zoekt up
    environment:
      # Index configuration
      ZOEKT_INDEX_DIR: /index
      ZOEKT_DATA_DIR: /data
    volumes:
      # Content directories to index:
      # - /data/cas/    = LOCAL backend CAS (content-addressable storage)
      # - /data/.cache/ = External connector L2 cache (GCS, S3, Gmail, etc.)
      # Note: Zoekt webserver needs write access for internal state
      - ./nexus-data:/data
      # Persistent trigram index storage
      - zoekt-index:/index
    ports:
      - "${ZOEKT_PORT:-6070}:6070"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:6070/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - nexus-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Zoekt indexer - runs once to build/update index
  # Uses zoekt-webserver image which includes zoekt-index binary
  zoekt-indexer:
    image: sourcegraph/zoekt-webserver:latest
    container_name: nexus-zoekt-indexer
    profiles:
      - zoekt-index  # Only runs with: docker compose --profile zoekt-index up
    volumes:
      # Content directories (need write for temp files)
      - ./nexus-data:/data
      - zoekt-index:/index
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Building Zoekt index..."
        echo "  - LOCAL backend: /data/cas/"
        echo "  - L2 cache: /data/.cache/"
        # Index all files in /data
        zoekt-index -index /index /data
        echo "Index build complete!"
        # Show stats
        ls -la /index/
    networks:
      - nexus-network

  # ============================================
  # DragonflyDB - High-Performance Cache (Issue #950)
  # Redis-compatible in-memory cache for:
  # - Embedding cache (reduces API calls by 90%)
  # - Permission cache
  # - Tiger cache
  # ============================================
  dragonfly:
    image: docker.dragonflydb.io/dragonflydb/dragonfly
    container_name: nexus-dragonfly
    restart: unless-stopped
    # Dragonfly optimizations:
    # --cache_mode=true: Smart eviction for cache workloads
    # --maxmemory=512mb: Memory limit for cache
    # --proactor_threads=2: Match small VM cores
    command: >
      --cache_mode=true
      --maxmemory=512mb
      --proactor_threads=2
    ports:
      - "${DRAGONFLY_PORT:-6379}:6379"
    volumes:
      - dragonfly-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    networks:
      - nexus-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

# ============================================
# Volumes
# ============================================
volumes:
  zoekt-index:
    driver: local
  postgres-data:
    driver: local
  dragonfly-data:
    driver: local
  # nexus-data: Using local directory ./nexus-data instead of Docker volume
  # This allows sharing data between Docker and local development

# ============================================
# Networks
# ============================================
networks:
  nexus-network:
    driver: bridge
