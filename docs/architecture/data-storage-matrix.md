# Data-to-Storage Properties Matrix

**Date:** 2026-02-12 (updated: sled ‚Üí redb throughout)
**Status:** Steps 1-3 COMPLETE ‚Äî All Data-Storage Affinity Decisions Resolved
**Purpose:** Catalog ALL data types in Nexus and determine optimal storage for each

---

## Methodology

Three-step **Data-Storage Affinity** analysis:

### Step 1: Data Layer
Eliminate or merge redundant data types based on **properties** and **use cases**.
- For each data type ask: "why does this exist?" and "is it redundant with another type?"
- Merge types that share the same properties and lifecycle (e.g. tenant‚Üízone, FilePathModel‚ÜíFileMetadata)

### Step 2: Storage Layer
Verify storage medium **orthogonality** ‚Äî no two stores should serve the same role.
- Each storage medium must have a unique capability profile
- Identify and deprecate redundant stores (e.g. Redis/Dragonfly post-Raft)

### Step 3: Affinity Matching
Map **data requiring properties** ‚Üî **storage providing properties**.
- Match each surviving data type to the storage medium whose properties best fit
- Result: each data type has exactly one canonical storage home

---

## Property Dimensions

| Property | Values | Meaning |
|----------|--------|---------|
| **Read Perf** | Low / Medium / High / Critical | Read query frequency & latency requirements |
| **Write Perf** | Low / Medium / High / Critical | Write frequency & latency requirements |
| **Consistency** | EC / SC / Strict SC | Eventual / Strong / Strict Strong Consistency |
| **Query Pattern** | KV / Relational / Vector / Blob | Access pattern (key-value, JOIN, similarity, large binary) |
| **Data Size** | Tiny / Small / Medium / Large / Huge | Typical size per record |
| **Cardinality** | Low / Medium / High / Very High | Number of records |
| **Durability** | Ephemeral / Session / Persistent / Archive | How long data must survive |
| **Scope** | System / Zone / User / Session | Isolation boundary |
| **Why Exists** | Brief rationale | First-principles justification |

---

## PART 1: CORE FILESYSTEM DATA

### 1.1 File Metadata (Primary)

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **FilePathModel** | High | Med | SC (multi) / Local (single) | Relational (JOIN on zone_id, FK lookups) | Small | Very High | Persistent | Zone | Map virtual path ‚Üí backend location; support multi-backend | SQLAlchemy | ~~Keep SQLAlchemy~~ ‚Üí **MERGE into FileMetadata (redb)** | ‚úÖ DECIDED: MERGE + DEPRECATE |
| **FileMetadata** (proto) | High | Med | SC (multi) / Local (single) | KV (by path) | Small | Very High | Persistent | Zone | Core file attributes (size, etag, timestamps) | Generated proto ‚Üí Python dataclass | **redb via Raft** (KV-friendly, SC via Raft) | ‚úÖ MIGRATE |
| **CompactFileMetadata** | Critical | Med | SC | KV | Tiny | Very High | Session | Zone | Memory-optimized metadata for L1 cache | In-memory (string interning) | **In-memory only** (cache layer) | ‚úÖ KEEP |

**Analysis (Step 1 DECIDED):**
- **FilePathModel ‚Üí FileMetadata**: ‚úÖ **MERGE CONFIRMED**. Deprecate relational model long-term.
  - FilePathModel has 17 columns but only 2 JOINs in codebase (cache invalidation + Tiger predicate pushdown)
  - Both JOINs replaceable: cache invalidation ‚Üí redb prefix scan, Tiger ‚Üí direct redb query
  - FK to FileMetadataModel (custom KV) ‚Üí redb prefix-keyed entries (`meta:{path_id}:{key}`)
  - No irreplaceable relational query exists on FilePathModel
- **CompactFileMetadata**: ‚úÖ **KEEP** ‚Äî Same 13 fields as FileMetadata but all strings interned to int IDs (cache-tier projection, ~64-100 bytes vs ~200-300 bytes at 1M+ scale). Already auto-generated from proto via `gen_metadata.py`.
- **FileMetadataModel (custom KV)**: ‚úÖ **KEEP SEPARATE** ‚Äî Arbitrary `{path_id, key, value}` pairs, fundamentally different from FileMetadata's fixed schema. Should NOT inherit from FileMetadata.

### 1.2 Directory Indexing

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **DirectoryEntryModel** | High | Low | SC | KV (by parent_path) | Small | High | Persistent | Zone | Sparse directory index for O(1) non-recursive `ls` | SQLAlchemy | ~~Separate redb entry~~ ‚Üí **MERGE into FileMetadata** | ‚úÖ DECIDED: MERGE |

**Analysis (Step 1+3 DECIDED):**
- Currently uses SQLAlchemy but access pattern is pure KV (lookup by parent_path)
- No JOINs needed ‚Üí Metastore (redb)
- **Step 3 merge decision**: In redb's ordered KV, directory listing = prefix scan on FileMetadata keys under `{parent_path}/`. One less data type. redb at ~14Œºs/op handles 1000-entry dirs in ~14ms.
- If future profiling shows large-directory bottleneck, re-introduce sparse index as Metastore-internal optimization (not a separate data type)

### 1.3 Custom Metadata

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|-----|-------|------------|----------------|-----------------|--------|
| **FileMetadataModel** (KV) | Med | Low | EC | KV (by path_id + key) | Small | Med | Persistent | Zone | Arbitrary user-defined metadata (tags, custom fields) | SQLAlchemy | **redb via Raft** (KV) | ‚úÖ MIGRATE |

**Analysis:**
- Pure KV access (lookup by path_id + key)
- No relational queries
- **Action**: Migrate to redb

---

## PART 2: CONTENT & DEDUPLICATION

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **ContentChunkModel** | Med | Low | EC | KV (by content_hash) | Small | High | Persistent | System | CAS (Content-Addressed Storage) for deduplication; track refcount | SQLAlchemy | **redb** (KV by hash, no Raft needed for CAS) | ‚úÖ MIGRATE |
| **File Content (blobs)** | Med | Low | EC | Blob (by path) | Huge | Very High | Persistent | Zone | Actual file data | Disk / S3 / GCS | **Keep Disk/S3** (blob storage) | ‚úÖ KEEP |
| **ContentCacheModel** | Med | Low | EC | KV (by path_id) | Large | High | Session | Zone | Parsed content cache (avoid re-parsing) | SQLAlchemy + Disk | **Disk only** (binary cache, no DB metadata needed) | ‚úÖ DECIDED: ELIMINATE DB |

**Analysis (Step 1 DECIDED):**
- **ContentChunkModel**: ‚úÖ Pure CAS, immutable ‚Üí move to redb (no Raft, just local KV)
- **ContentCacheModel**: ‚úÖ **ELIMINATE DB metadata**, simplify to pure disk cache with TTL. No DB model needed.

---

## PART 3: VERSIONING

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **VersionHistoryModel** | Low | Low | EC | Relational (parent_version_id FK) | Small | High | Archive | Zone | Track file/memory/skill version history | SQLAlchemy with BRIN index | **Keep SQLAlchemy** (needs parent FK, BRIN for time-series) | ‚úÖ KEEP |
| **WorkspaceSnapshotModel** | Low | Low | EC | Relational (FK to snapshot files) | Small | Low | Archive | Zone | Point-in-time workspace captures (zero-copy via CAS) | SQLAlchemy | **Keep SQLAlchemy** (relational queries for snapshot browsing) | ‚úÖ KEEP |

**Analysis:**
- Both have relational queries (parent FK, time-series)
- Low frequency ‚Üí PostgreSQL BRIN indexes work well
- **Action**: Keep in SQLAlchemy

---

## PART 4: MEMORY SYSTEM (ACE)

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **MemoryModel** | Med | Med | EC | Relational + Vector (embedding search, entity/relationship queries, decay tracking) | Medium | Very High | Persistent | User/Agent | AI agent memory with identity-based 3-layer permissions; supports semantic search, entity extraction, temporal refs, ACE consolidation | SQLAlchemy with BRIN + vector index (pgvector/sqlite-vec) | **Keep SQLAlchemy** (complex relational + vector queries) | ‚úÖ KEEP |
| **MemoryConfig** | Low | Low | EC | KV (by path) | Tiny | Low | Persistent | Zone | Memory directory configuration | In-memory + SQLAlchemy | ~~redb~~ ‚Üí **Keep RecordStore** (co-existence with MemoryModel) | ‚úÖ DECIDED: STAY RecordStore |
| **TrajectoryModel** | Low | Med | EC | Relational (FK to agent, task) | Small | High | Persistent | Agent | Task execution traces for ACE learning | Inferred (implicit in memory system) | **Keep SQLAlchemy** (relational) | ‚úÖ KEEP |
| **TrajectoryFeedbackModel** | Low | Low | EC | Relational (FK to trajectory) | Small | Med | Persistent | Agent | Feedback on trajectories | SQLAlchemy | **Keep SQLAlchemy** (FK to trajectory) | ‚úÖ KEEP |
| **PlaybookModel** | Low | Low | EC | Relational (FK to strategies) | Medium | Med | Persistent | Agent | Strategy playbooks | Inferred (API models) | **Keep SQLAlchemy** (relational) | ‚úÖ KEEP |

**Analysis (Step 1 DECIDED):**
- **MemoryModel**: ‚úÖ KEEP RecordStore ‚Äî complex relational + vector queries (pgvector)
- **MemoryConfig**: ‚úÖ **KEEP RecordStore** (was: migrate to redb). **Cross-pillar co-existence principle**: MemoryConfig is meaningless without MemoryModel. If RecordStore is not injected, orphaned MemoryConfig entries in Metastore would point to non-functional memory. Configs that only serve RecordStore data belong in RecordStore.
- Trajectory/Playbook: ‚úÖ KEEP RecordStore ‚Äî relational FK
- **No merges needed** within this part ‚Äî all serve distinct purposes

---

## PART 5: ACCESS CONTROL (ReBAC)

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **ReBACTupleModel** | Critical | Low | SC | Relational (composite index on subject/relation/object) | Tiny | Very High | Persistent | Zone | Zanzibar-style relationship tuples (user:alice#member@group:eng) | SQLAlchemy with composite indexes | **RecordStore** (SSOT) + **CacheStore** (hot path) | ‚úÖ DECIDED |
| **ReBACNamespaceModel** | Med | Low | EC | KV (by namespace_id) | Small | Low | Persistent | System | Permission expansion rules (namespace config) | SQLAlchemy | **redb** (KV, low cardinality) | ‚úÖ MIGRATE |
| **ReBACGroupClosureModel** | Critical | Low | SC | Relational (composite index on member/group) | Tiny | Very High | Persistent | Zone | Leopard-style transitive closure for O(1) group membership | SQLAlchemy with composite indexes | **Keep SQLAlchemy** (critical path, materialized view) | ‚úÖ KEEP |
| **ReBACChangelogModel** | Low | Med | EC | Relational (BRIN index on created_at) | Small | High | Archive | Zone | Audit log for tuple modifications | SQLAlchemy with BRIN | **Keep SQLAlchemy** (append-only, BRIN optimized) | ‚úÖ KEEP |

**Analysis (Step 1+3 DECIDED):**
- **Layering**: ReBAC is a **service** (user management), NOT kernel.
- **No merges needed** ‚Äî Zanzibar-correct: TupleModel (SSOT), GroupClosureModel (derived), ChangelogModel (audit), NamespaceModel (config)
- **ReBACTupleModel affinity (Step 3)**:
  - Required: composite index (6-field), SC, persistent, critical read path
  - Ordered KV (Metastore): ‚úÖ fast (~14Œºs), ‚úÖ SC (Raft), but ‚ùå composite indexes must be hand-encoded as prefix keys + secondary index key patterns ‚Äî reimplements what SQL gives for free
  - Relational ACID (RecordStore): ‚úÖ composite indexes native, ‚úÖ SC (ACID), ‚úÖ persistent, but ‚ö†Ô∏è ~1ms latency
  - **Decision**: **RecordStore** (SSOT) ‚Äî composite indexes are the dominant requirement. Hot-path latency solved by CacheStore (TigerCache + PermissionCache already exist as caching layer).
  - ‚ö†Ô∏è **Architecture risk**: Permission hot path depends on CacheStore. If CacheStore unavailable, falls back to ~1ms SQL. Acceptable ‚Äî CacheStore is optional optimization, not correctness requirement.

---

## PART 6: USERS & AUTHENTICATION

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **UserModel** | Med | Low | SC | Relational (JOIN on zone_id, email lookup) | Small | Med | Persistent | System | Core user accounts with soft delete | SQLAlchemy with soft delete | **Keep SQLAlchemy** (relational queries) | ‚úÖ KEEP |
| **UserOAuthAccountModel** | Med | Low | SC | Relational (FK to user_id, unique constraint on provider+provider_user_id) | Small | Med | Persistent | System | OAuth provider accounts for SSO login | SQLAlchemy | **Keep SQLAlchemy** (FK, unique constraints) | ‚úÖ KEEP |
| **OAuthCredentialModel** | Med | Low | SC | Relational (FK to user_id, zone_id, encrypted tokens) | Small | Med | Persistent | Zone | OAuth tokens for backend integrations (Google Drive, OneDrive) | SQLAlchemy with encryption | **Keep SQLAlchemy** (FK, encryption) | ‚úÖ KEEP |
| **UserSessionModel** | High | Med | EC | KV (by session_id) | Tiny | High | Session | System | Active user sessions | SQLAlchemy | **CacheStore** (Dragonfly / In-Memory) | ‚úÖ DECIDED: CacheStore |

**Analysis (Step 1+3 DECIDED):**
- **No merges or abstractions needed** ‚Äî well-designed, minimal redundancy:
  - **UserOAuthAccountModel** vs **OAuthCredentialModel**: Intentionally separate ‚Äî *login auth* (ID token only) vs *backend integration* (access/refresh tokens). Different security flows.
  - User/OAuth models: ‚úÖ KEEP RecordStore ‚Äî relational queries, FK, encryption
- **UserSessionModel affinity (Step 3)**:
  - Required: KV by session_id, TTL expiry, high read freq, EC sufficient
  - Relational ACID (RecordStore): ‚úÖ works, but ‚ùå no native TTL, ‚ùå overkill (no JOINs/FK needed)
  - Ephemeral KV (CacheStore): ‚úÖ KV native, ‚úÖ TTL native, ‚úÖ high read perf, ‚úÖ EC
  - **Decision**: **CacheStore** ‚Äî pure KV with TTL, no relational features needed
  - Admin queries ("all sessions for user X") use CacheStore scan (rare, acceptable latency)

---

## PART 7: ZONES & ISOLATION

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **ZoneModel** | Med | Low | SC | Relational (unique constraint on domain) | Small | Low | Persistent | System | Zone/organization metadata with soft delete | SQLAlchemy with soft delete | **Keep SQLAlchemy** (unique constraint, soft delete) | ‚úÖ KEEP |
| **EntityRegistryModel** | Med | Low | SC | Relational (parent_type/parent_id FK) | Tiny | Med | Persistent | System | Identity hierarchy (zone‚Üíuser‚Üíagent) | SQLAlchemy | **Keep SQLAlchemy** (hierarchical FK) | ‚úÖ KEEP |
| **ExternalUserServiceModel** | Low | Low | EC | Relational (encrypted config) | Small | Low | Persistent | System | External user management config | SQLAlchemy with encryption | **Keep SQLAlchemy** (encryption) | ‚úÖ KEEP |

**Analysis:**
- All need relational features (unique constraints, FK, encryption)
- **Action**: Keep SQLAlchemy

---

## PART 8: EVENTS & SUBSCRIPTIONS

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **FileEvent** | N/A | High | EC | Pub/Sub | Tiny | N/A | Ephemeral | Zone | File change notifications (write, delete, rename) | In-memory ‚Üí Dragonfly pub/sub | **CacheStore** (pub/sub) | ‚úÖ DECIDED: CacheStore |
| **SubscriptionCreate/Update** | Med | Low | EC | Relational (FK to zone, query by event_types) | Small | Low | Persistent | Zone | Webhook subscription config | Pydantic (API only, no DB model found) | **Need SQLAlchemy model?** | ‚ùì MISSING |
| **WebhookDelivery** | Low | Med | EC | Relational (BRIN on created_at) | Small | High | Archive | Zone | Webhook delivery attempt history | Pydantic (API only) | **Need SQLAlchemy model?** | ‚ùì MISSING |

**Analysis (Step 1+3 DECIDED):**
- **No merges** ‚Äî different lifecycles (ephemeral / persistent config / audit log), pipeline: Subscription + FileEvent match ‚Üí WebhookDelivery
- **Co-location**: Subscription and WebhookDelivery ‚Üí RecordStore (both persistent, relational)
- **FileEvent affinity (Step 3)**:
  - Required: pub/sub (publish to channel, subscribers receive), ephemeral, high write freq, EC
  - Ordered KV (Metastore): ‚ùå no pub/sub ‚Äî would need polling, defeats purpose of event-driven
  - Ephemeral KV + Pub/Sub (CacheStore): ‚úÖ pub/sub native, ‚úÖ ephemeral, ‚úÖ high throughput, ‚úÖ EC
  - **Decision**: **CacheStore** ‚Äî pub/sub is the dominant requirement. Events are fire-and-forget notifications; missed events can be recovered from SSOT (Metastore).
  - ‚ö†Ô∏è **Gap**: EventBusProtocol currently has NO in-memory impl. Need `InMemoryEventBus` for kernel-only/dev mode.
- **Subscription/Delivery** DB models: ‚ùì STILL MISSING ‚Äî need RecordStore models (Task #12)

---

## PART 9: WORKFLOWS

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **WorkflowModel** | Med | Low | EC | Relational (FK to zone, version tracking) | Medium | Low | Persistent | Zone | Workflow definitions (YAML) | SQLAlchemy | **Keep SQLAlchemy** (version tracking, FK) | ‚úÖ KEEP |
| **WorkflowExecutionModel** | Med | Med | EC | Relational (FK to workflow, BRIN on started_at) | Small | High | Archive | Zone | Workflow execution history | SQLAlchemy with BRIN | **Keep SQLAlchemy** (append-only, BRIN) | ‚úÖ KEEP |

**Analysis:**
- Relational queries needed
- **Action**: Keep SQLAlchemy

---

## PART 10: SEMANTIC SEARCH

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **DocumentChunkModel** | Med | Med | EC | Vector (embedding similarity search) | Medium | Very High | Persistent | Zone | Document chunks with embeddings for semantic search | SQLAlchemy with pgvector/sqlite-vec | **Keep SQLAlchemy** (vector indexes) | ‚úÖ KEEP |

**Analysis:**
- Requires vector index (pgvector for PostgreSQL, sqlite-vec for SQLite)
- **Action**: Keep SQLAlchemy

---

## PART 11: AUDIT & LOGGING

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **OperationLogModel** | Low | High | EC | Relational (BRIN on created_at) | Small | Very High | Archive | Zone | Filesystem operation audit trail | SQLAlchemy with BRIN | **Keep SQLAlchemy** (append-only, BRIN optimized) | ‚úÖ KEEP |

**Analysis:**
- Append-only log with time-series queries
- **Action**: Keep SQLAlchemy with BRIN

---

## PART 12: SANDBOXES

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **SandboxMetadataModel** | Med | Med | EC | Relational (FK to user/agent/zone, status queries) | Small | Med | Persistent | Zone | Managed sandbox instances (E2B, Docker, Modal) | SQLAlchemy | **Keep SQLAlchemy** (relational queries) | ‚úÖ KEEP |

---

## PART 13: SYSTEM CONFIGURATION

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **SystemSettingsModel** | Med | Low | SC | KV (by key) | Small | Low | Persistent | System | System-wide settings (OAuth encryption key, feature flags) | SQLAlchemy | **redb** (KV, low cardinality) | ‚úÖ MIGRATE |
| ~~**Cluster Topology**~~ | ~~Med~~ | ~~Low~~ | ~~SC~~ | ~~???~~ | ~~Small~~ | ~~Low~~ | ~~Persistent~~ | ~~System~~ | ~~Raft cluster membership, node addresses~~ | ~~???~~ | N/A | ‚úÖ DECIDED: ELIMINATE |

**Analysis (Step 1 DECIDED):**
- **SystemSettingsModel**: ‚úÖ Pure KV ‚Üí keep in Metastore (redb). No merge needed.
- **Cluster Topology**: ‚úÖ **ELIMINATED** as standalone data type. Raft node membership is inherent in the Raft consensus layer's own log (redb). If no Raft service ‚Üí doesn't exist. Not application-level data.

---

## PART 14: CACHE LAYERS

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **PermissionCacheProtocol** | Critical | Med | EC | KV (by cache key) | Tiny | Very High | Session | Zone | Permission check result cache (avoid ReBAC recomputation) | Dragonfly/PostgreSQL/In-memory | **Dragonfly** (in-memory, TTL) | ‚úÖ KEEP |
| **TigerCacheProtocol** | Critical | Low | EC | KV (by object_id ‚Üí bitmap) | Small | High | Session | Zone | Pre-materialized permission bitmaps for O(1) filtering | Dragonfly/PostgreSQL | **Dragonfly** (in-memory, fast bitmap ops) | ‚úÖ KEEP |

**Analysis:**
- Both are performance caches, not SSOT
- **Action**: Keep Dragonfly (in-memory cache)

---

## PART 15: WORKSPACE & MEMORY CONFIG

| Data Type | Read | Write | Consistency | Query | Size | Card | Dur | Scope | Why Exists | Current Storage | Optimal Storage | Action |
|-----------|------|-------|-------------|-------|------|------|-----|-------|------------|----------------|-----------------|--------|
| **WorkspaceConfig** | Low | Low | EC | KV (by path) | Small | Low | Persistent | Zone | Workspace directory registration | In-memory + SQLAlchemy | **MERGE ‚Üí PathRegistrationModel** | ‚úÖ DECIDED: MERGE |
| **MemoryConfigModel** (DB) | Low | Low | EC | KV (by path) | Small | Low | Persistent | Zone | Memory directory configuration (DB storage) | SQLAlchemy | **MERGE ‚Üí PathRegistrationModel** | ‚úÖ DECIDED: MERGE |

**Analysis (Step 1 DECIDED):**
- ‚úÖ **MERGE into single `PathRegistrationModel`** with `type` discriminator ("workspace" | "memory")
- Schemas are structurally identical: path, name, description, created_at, created_by, metadata (+ DB extras: user_id, agent_id, scope, session_id, expires_at)
- **Storage**: RecordStore (not Metastore) ‚Äî same co-existence principle as MemoryConfig: WorkspaceConfig is meaningless without WorkspaceSnapshotModel in RecordStore

---

## SUMMARY: STORAGE LAYER DECISIONS

### ‚úÖ **Keep SQLAlchemy (PostgreSQL/SQLite) = RecordStore** - 22 types (was 20, +2 from co-existence moves)
Relational queries, FK, unique constraints, vector search, encryption, BRIN indexes

| Category | Data Types | Rationale |
|----------|-----------|-----------|
| **Users & Auth** | UserModel, UserOAuthAccountModel, OAuthCredentialModel | Relational queries, FK, unique constraints, encryption |
| **ReBAC (Partial)** | ReBACGroupClosureModel, ReBACChangelogModel | Materialized view, append-only BRIN |
| **Memory System** | MemoryModel, **MemoryConfig**, TrajectoryModel, TrajectoryFeedbackModel, PlaybookModel | Complex relational + vector search; MemoryConfig co-exists with MemoryModel |
| **Versioning** | VersionHistoryModel, WorkspaceSnapshotModel | Parent FK, BRIN time-series |
| **Semantic Search** | DocumentChunkModel | Vector index (pgvector/sqlite-vec) |
| **Workflows** | WorkflowModel, WorkflowExecutionModel | Version tracking, FK, BRIN |
| **Zones** | ZoneModel, EntityRegistryModel, ExternalUserServiceModel | Unique constraints, hierarchical FK, encryption |
| **Audit** | OperationLogModel | Append-only BRIN |
| **Sandboxes** | SandboxMetadataModel | Relational queries |
| **Path Registration** | **PathRegistrationModel** (NEW: WorkspaceConfig + MemoryConfig merged) | Co-exists with SnapshotModel/MemoryModel |

### ‚úÖ **Metastore (Ordered KV ‚Äî redb via Raft)** ‚Äî 4 surviving types
KV access pattern, strong consistency needed (multi-node)

| Data Type | Current | Reason |
|-----------|---------|--------|
| **FileMetadata** (proto) + ~~FilePathModel~~ + ~~DirectoryEntryModel~~ | Generated dataclass / SQLAlchemy | Core metadata KV by path; FilePathModel + DirectoryEntry merged in; dir listing = prefix scan |
| FileMetadataModel (custom KV) | SQLAlchemy | Arbitrary KV metadata by path_id + key |
| ReBACNamespaceModel | SQLAlchemy | KV by namespace_id, low cardinality |
| SystemSettingsModel | SQLAlchemy | KV by key, low cardinality |

### ‚úÖ **Migrate to redb (local, no Raft)** - 1 type
CAS (content-addressed), immutable

| Data Type | Current | Reason |
|-----------|---------|--------|
| ContentChunkModel | SQLAlchemy | KV by content_hash, immutable (no SC needed) |

### ‚úÖ **CacheStore (Ephemeral KV ‚Äî Dragonfly / In-Memory)** ‚Äî 4 types
Performance cache, TTL, pub/sub

| Data Type | Current | Reason |
|-----------|---------|--------|
| PermissionCacheProtocol | Dragonfly/PostgreSQL/In-memory | Permission check cache, TTL |
| TigerCacheProtocol | Dragonfly/PostgreSQL | Pre-materialized bitmaps, TTL |
| **FileEvent** (pub/sub) | Dragonfly pub/sub | Ephemeral change notifications, pub/sub native |
| **UserSessionModel** | SQLAlchemy | Pure KV with TTL, no relational features needed |

### ‚úÖ **Step 1 DECISIONS RESOLVED**

1. ‚úÖ **FilePathModel ‚Üí FileMetadata**: MERGE confirmed, deprecate relational model
2. ‚úÖ **ContentCacheModel**: ELIMINATE DB metadata, pure disk cache
3. ‚úÖ **Cluster Topology**: ELIMINATED as standalone type (inherent in Raft layer)
4. ‚úÖ **WorkspaceConfig + MemoryConfig**: MERGE into PathRegistrationModel (RecordStore)
5. ‚úÖ **MemoryConfig pillar**: STAY RecordStore (cross-pillar co-existence principle)
6. ‚úÖ **ReBAC 4 types**: No merges needed (Zanzibar-correct)
7. ‚úÖ **User/Auth types**: No merges needed
8. ‚úÖ **Events/Subscriptions**: No merges, co-locate Subscription+Delivery in RecordStore

### ‚úÖ **Step 3 DECISIONS RESOLVED (Affinity Matching)**

1. ‚úÖ **ReBACTupleModel ‚Üí RecordStore** (SSOT) + CacheStore (hot path). Composite indexes are dominant requirement; hot-path latency covered by TigerCache/PermissionCache.
2. ‚úÖ **FileEvent ‚Üí CacheStore** (pub/sub). Ephemeral, fire-and-forget; missed events recoverable from Metastore SSOT.
3. ‚úÖ **UserSessionModel ‚Üí CacheStore**. Pure KV with TTL, no relational features needed.
4. ‚úÖ **DirectoryEntryModel ‚Üí MERGE into FileMetadata** (Metastore). Prefix scan replaces sparse index; one less data type.

### ‚ö†Ô∏è **Architecture Risks Identified (Step 3)**

1. **CacheStore dependency for permissions**: ReBAC hot path (TigerCache, PermissionCache) depends on CacheStore. If CacheStore unavailable, falls back to ~1ms SQL (RecordStore). Acceptable ‚Äî optimization, not correctness.
2. **Missing InMemoryEventBus**: EventBusProtocol has Dragonfly impl but NO in-memory impl. Kernel-only/dev mode has no event bus. Need `InMemoryEventBus` for CacheStoreABC.
3. **Missing InMemory impls**: PermissionCache and TigerCache also lack in-memory impls. Same CacheStoreABC gap.

### ‚ùì **REMAINING GAPS**

1. **Subscription/Delivery DB models**: Pydantic models exist, need RecordStore models (Task #12)
2. **CacheStoreABC + InMemoryCacheStore**: Need to implement for kernel-only/dev fallback (Task #22)

---

## REDUNDANCY ANALYSIS (Step 1 Complete)

### ‚úÖ Confirmed MERGES:

1. **FilePathModel + FileMetadata** ‚Üí ‚úÖ **MERGE into FileMetadata (redb)**
   - FilePathModel 17 columns, only 2 JOINs (both replaceable with redb prefix scan)
   - Deprecate relational model long-term

2. **WorkspaceConfig + WorkspaceConfigModel + MemoryConfig + MemoryConfigModel** ‚Üí ‚úÖ **MERGE into PathRegistrationModel (RecordStore)**
   - All 4 types have identical schemas (path, name, description, created_at, created_by, metadata)
   - Single model with `type` discriminator ("workspace" | "memory")
   - Lives in RecordStore (co-existence principle: meaningless without SnapshotModel/MemoryModel)

3. **Cluster Topology** ‚Üí ‚úÖ **ELIMINATED**
   - Not application-level data; inherent in Raft consensus layer
   - If no Raft ‚Üí doesn't exist

4. **ContentCacheModel** ‚Üí ‚úÖ **ELIMINATE DB metadata**
   - Simplify to pure disk cache with TTL, no SQLAlchemy model needed

### ‚úÖ Confirmed NO-MERGE (architecture is correct):

5. **ReBAC 4 types** ‚Äî Zanzibar-correct: SSOT (Tuple, Namespace), Derived (GroupClosure), Audit (Changelog)
6. **User/Auth 4 types** ‚Äî Clean separation: identity (User), login auth (OAuthAccount), backend integration (OAuthCredential), sessions (UserSession)
7. **Events 3 types** ‚Äî Different lifecycles: ephemeral (FileEvent), persistent config (Subscription), audit (Delivery)
8. **CompactFileMetadata** ‚Äî Cache-tier projection of FileMetadata (auto-generated from proto)
9. **FileMetadataModel (custom KV)** ‚Äî Arbitrary user-defined pairs, fundamentally different from fixed-schema FileMetadata

### üÜï New Principle: Cross-Pillar Co-existence

> **If a config type only exists to serve data in another pillar, it belongs in that pillar.**
>
> MemoryConfig is meaningless without MemoryModel (RecordStore). WorkspaceConfig is meaningless without WorkspaceSnapshotModel (RecordStore). Therefore both belong in RecordStore, not Metastore, despite their KV access pattern.

---

## STORAGE MEDIUM ORTHOGONALITY ANALYSIS (Step 2 ‚Äî DECIDED)

### Core Insight: Storage Mediums = Pillars, not Implementations

Orthogonality analysis operates at the **storage medium** level, not the driver/implementation level.
Drivers within the same pillar are interchangeable (deployment-time config via ABC), not architectural choices.

> **Principle**: If two "storage mediums" serve the same query pattern and are abstracted behind the same ABC,
> they are **drivers** of one medium, not separate mediums.

This collapses 9 listed implementations ‚Üí **4 storage mediums** (1:1 with Four Pillars):

| Pillar | Storage Medium | Unique Properties | Drivers (interchangeable via ABC) |
|--------|---------------|-------------------|-----------------------------------|
| **Metastore** | Ordered KV | Persistent, ordered prefix scan, optional Raft SC, ~14Œºs ops | redb (local PyO3), redb (gRPC Raft) |
| **RecordStore** | Relational ACID | JOINs, FK, unique constraints, vector search, BRIN indexes | PostgreSQL (networked, multi-writer), SQLite (embedded, single-writer) |
| **ObjectStore** | Blob | Streaming I/O, petabyte scale, content-addressed | S3, GCS, Azure Blob (cloud), Local Disk (embedded) |
| **CacheStore** | Ephemeral KV + Pub/Sub | TTL, pub/sub, no persistence guarantee | Dragonfly (networked), In-Memory (process-local: Python dict / DashMap) |

### Kernel Self-Inclusiveness Check

With kernel-only (Metastore required + ObjectStore required, no services):

| Kernel need | Provided by | Storage property used |
|-------------|------------|----------------------|
| File metadata (inode) | Metastore (redb) | KV by path |
| Directory index (dentry) | Metastore (redb) | Ordered prefix scan |
| Zone revision tracking | Metastore (redb) | `/__sys__/` KV entries |
| System settings | Metastore (redb) | KV by key |
| File content (bytes) | ObjectStore (Backend) | Blob by path |

Kernel does NOT need: JOINs, FK, vector search, BRIN, TTL, pub/sub, composite indexes.
Those are all service-layer concerns (RecordStore/CacheStore).

**Verdict**: ‚úÖ Kernel is **self-inclusive** with 2 storage mediums (Ordered KV + Blob). Zero unnecessary properties.

CompactFileMetadata (DashMap L1 cache) is process-internal optimization, not a storage medium ‚Äî like a CPU cache.

### Orthogonality Between Pillars (4 mediums)

#### ‚úÖ Ordered KV (Metastore) vs Relational ACID (RecordStore)
- **Metastore**: Pure KV, ordered prefix scan, ~14Œºs, no JOINs, no FK
- **RecordStore**: JOINs, FK, unique constraints, vector search, BRIN indexes, ~1ms
- **Verdict**: **Orthogonal** ‚Äî fundamentally different query patterns (KV vs relational)

#### ‚úÖ Ordered KV (Metastore) vs Ephemeral KV (CacheStore)
- **Metastore**: Persistent SSOT, linearizable (Raft), embedded
- **CacheStore**: Ephemeral cache, eventual consistency, TTL eviction, pub/sub
- **Verdict**: **Orthogonal** ‚Äî different durability (persistent vs ephemeral) and consistency guarantees

#### ‚úÖ Relational ACID (RecordStore) vs Blob (ObjectStore)
- **RecordStore**: Structured data, small records, complex queries
- **ObjectStore**: Unstructured bytes, huge objects, no queries (by-key only)
- **Verdict**: **Orthogonal** ‚Äî different data shape (structured vs unstructured)

#### ‚úÖ Ephemeral KV (CacheStore) vs Blob (ObjectStore)
- **CacheStore**: Tiny KV entries, TTL, pub/sub, in-memory
- **ObjectStore**: Huge blobs, persistent, streaming I/O
- **Verdict**: **Orthogonal** ‚Äî different size profile and durability

### Driver Merges Within Pillars (Step 2 decisions)

#### ‚ùå **DEPRECATE Redis** ‚Üí merge into Dragonfly (CacheStore driver)
- Same storage medium (Ephemeral KV + Pub/Sub), same protocol
- Dragonfly: 25x memory efficiency, multi-threaded, drop-in replacement
- **Migration**: Change connection string only, zero code changes

#### ‚úÖ **MERGE In-Memory Python dict + DashMap** ‚Üí single "In-Memory" driver (CacheStore)
- Same storage medium: process-local ephemeral KV, no persistence, no TTL
- DashMap is a faster engine (~100ns vs ~1Œºs), not a different medium
- Under CacheStoreABC: `InMemoryCacheStore(engine="dict")` vs `InMemoryCacheStore(engine="dashmap")`

#### ‚úÖ **PostgreSQL + SQLite** are drivers, not separate mediums (RecordStore)
- Same query patterns (SQL, JOINs, FK, ACID), same ABC (RecordStoreABC via SQLAlchemy)
- Difference is operational (networked multi-writer vs embedded single-writer), not architectural
- Driver selection is deployment-time configuration, not storage architecture

#### ‚úÖ **S3/GCS/Azure + Local Disk** are drivers, not separate mediums (ObjectStore)
- Same access pattern (blob by key, streaming I/O), same ABC (ObjectStoreABC = Backend)
- Difference is operational (cloud managed vs local embedded)

### Storage Medium Properties Matrix (4 mediums)

| Medium | Read Perf | Write Perf | Consistency | Query Patterns | Durability | Unique Capability |
|--------|-----------|------------|-------------|----------------|------------|-------------------|
| **Ordered KV** | Critical (~14Œºs) | Critical (~14Œºs) | Linearizable (Raft) / Local | Ordered KV, prefix scan, range queries | Persistent (B+ tree) | **Ordered iteration** for user root localization (first key = `/` in chroot) |
| **Relational ACID** | Med (~1ms) | Med (~1ms) | Serializable (ACID) | JOIN, FK, vector (pgvector), BRIN | Persistent (WAL) | **Complex queries** ‚Äî JOINs, referential integrity, vector similarity search |
| **Blob** | Med (variable) | Med (variable) | Eventual / Local | By-key only, streaming I/O | Persistent (11-nines) | **Unbounded size** ‚Äî petabyte-scale object storage |
| **Ephemeral KV** | Critical (<1Œºs) | Critical (<1Œºs) | Eventual / Local | KV + pub/sub + TTL | Ephemeral (lost on restart) | **TTL + pub/sub** ‚Äî cache invalidation, event bus, session management |

### Deployment Mode ‚Üí Driver Selection

| Deployment Mode | RecordStore driver | Metastore driver | ObjectStore driver | CacheStore driver |
|-----------------|-------------------|------------------|-------------------|-------------------|
| **Dev (single-node)** | SQLite | redb (local) | Local Disk | In-Memory (dict/DashMap) |
| **Production (single-node)** | PostgreSQL | redb (local) | S3 / Local | Dragonfly |
| **Production (multi-node)** | PostgreSQL | redb (Raft) | S3 | Dragonfly |

### Key Insights

1. **4 storage mediums, 1:1 with Four Pillars**: Orthogonality is between pillars (different query patterns), not between drivers within a pillar (same pattern, different operational profiles).

2. **Kernel needs exactly 2 mediums**: Ordered KV (Metastore) + Blob (ObjectStore). Services optionally add Relational ACID (RecordStore) and/or Ephemeral KV (CacheStore). Kernel is self-inclusive.

3. **Drivers are deployment-time config**: PostgreSQL vs SQLite, S3 vs Local Disk, Dragonfly vs In-Memory ‚Äî all selected by deployment context, abstracted behind ABCs.

4. **3 driver merges**: Redis ‚Üí Dragonfly (redundant), In-Memory dict + DashMap ‚Üí single driver with engine selection, PostgreSQL + SQLite ‚Üí conceptually one medium.

### Action Items

1. ‚úÖ **Step 2 COMPLETE**: 4 orthogonal storage mediums verified (1:1 with Pillars)
2. ‚ö†Ô∏è **Deprecate Redis** (P2): Merge into Dragonfly driver (change connection string only)
3. ‚úÖ **Kernel self-inclusiveness verified**: 2 mediums sufficient (Ordered KV + Blob)
4. ‚úÖ **New principle**: Orthogonality = between pillars; drivers = within pillars

---

## THE NEXUS QUARTET: FOUR STORAGE PILLARS (Task #14)

**Design Decision**: NexusFS (nexus-core) abstracts storage by **Capability** (Access Pattern & Consistency Guarantee),
not by domain (`UserStore`) or implementation (`PostgresStore`).
Inspired by Linux Kernel's `BlockDevice`/`CharDevice`/`FileSystem` model.
Names explain the **"What"** and **"Why"**, not the **"How"**.

### The Four Pillars

| Pillar | ABC | Role | Backing Drivers | Kernel Status |
|--------|-----|------|-----------------|---------------|
| **Metastore** | `MetastoreABC` | "The Structure" ‚Äî inodes, dentries, config, topology | redb (local PyO3 / gRPC Raft) | **Required** init param |
| **RecordStore** | `RecordStoreABC` | "The Truth" ‚Äî entities, relationships, logs, vectors | PostgreSQL (prod), SQLite (dev) | **Optional** ‚Äî injected for Services |
| **ObjectStore** | `ObjectStoreABC` (= current `Backend`) | "The Content" ‚Äî raw file bytes, immutable objects | S3, GCS, Local Disk | **Mounted** dynamically (like Linux `mount`) |
| **CacheStore** | `CacheStoreABC` (future) | "The Reflexes" ‚Äî sessions, signals, ephemeral data | Dragonfly (prod), In-Memory (dev) | **Future** ‚Äî optional |

**Naming Note**: The existing proto-generated `MetadataStore` (specific to `FileMetadata` typed operations)
will be renamed to `FileMetadataProtocol` to avoid confusion with `MetastoreABC` (the underlying ordered KV primitive).
`MetastoreABC` is the lower-level KV store; `FileMetadataProtocol` is a typed wrapper that sits on top of it.

### Complete Data Type ‚Üí Pillar Mapping

**Metastore** (Ordered KV ‚Äî redb):
| Data Type | From Part | Rationale |
|-----------|-----------|-----------|
| **FileMetadata** (+ merged FilePathModel, DirectoryEntryModel) | Part 1 | Core file attributes, KV by path. Dir listing = prefix scan. |
| FileMetadataModel (custom KV) | Part 1 | Arbitrary user metadata, KV by path_id + key |
| ContentChunkModel | Part 2 | CAS dedup index, KV by content_hash (immutable, local only) |
| ReBACNamespaceModel | Part 5 | Permission config, KV by namespace_id |
| SystemSettingsModel | Part 13 | System config, KV by key |

**RecordStore** (Relational ‚Äî PostgreSQL/SQLite):
| Data Type | From Part | Rationale |
|-----------|-----------|-----------|
| UserModel, UserOAuthAccountModel, OAuthCredentialModel | Part 6 | FK, unique constraints, encryption |
| ReBACTupleModel, ReBACGroupClosureModel, ReBACChangelogModel | Part 5 | Composite indexes, materialized view, BRIN |
| MemoryModel, MemoryConfig, TrajectoryModel, TrajectoryFeedbackModel, PlaybookModel | Part 4 | Vector search (pgvector), relational FK; MemoryConfig co-exists with MemoryModel |
| VersionHistoryModel, WorkspaceSnapshotModel | Part 3 | Parent FK, BRIN time-series |
| DocumentChunkModel | Part 10 | Vector index (pgvector/sqlite-vec) |
| WorkflowModel, WorkflowExecutionModel | Part 9 | Version tracking, FK, BRIN |
| ZoneModel, EntityRegistryModel, ExternalUserServiceModel | Part 7 | Unique constraints, hierarchical FK |
| OperationLogModel | Part 11 | Append-only BRIN |
| SandboxMetadataModel | Part 12 | Relational queries |
| **PathRegistrationModel** (NEW: merged WorkspaceConfig + MemoryConfig) | Part 15 | Co-exists with SnapshotModel/MemoryModel (cross-pillar principle) |

**ObjectStore** (= existing `Backend` ABC ‚Äî S3/Local Disk):
| Data Type | From Part | Rationale |
|-----------|-----------|-----------|
| File Content (blobs) | Part 2 | Actual file bytes, petabyte scale, streaming I/O |

**CacheStore** (Ephemeral KV + Pub/Sub ‚Äî Dragonfly / In-Memory):
| Data Type | From Part | Rationale |
|-----------|-----------|-----------|
| UserSessionModel | Part 6 | Session tokens, pure KV with TTL (Step 3 decided) |
| PermissionCacheProtocol | Part 14 | Permission check cache, TTL |
| TigerCacheProtocol | Part 14 | Pre-materialized bitmaps, TTL |
| FileEvent (pub/sub) | Part 8 | Ephemeral change notifications, pub/sub (Step 3 decided) |

### CacheStore Implementation Status

‚ö†Ô∏è **GAP**: Existing impls are scattered and lack in-memory fallbacks for kernel-only/dev mode:
- **EventBus**: `EventBusProtocol` (ABC), `RedisEventBus` (Dragonfly impl) ‚Äî ‚ùå NO in-memory impl
- **PermissionCache**: `PermissionCacheProtocol` (ABC), `DragonflyPermissionCache`, `PostgresPermissionCache` ‚Äî ‚ùå NO in-memory impl
- **TigerCache**: `TigerCacheProtocol` (ABC), `DragonflyTigerCache`, `PostgresTigerCache` ‚Äî ‚ùå NO in-memory impl
- **UserSession**: Currently in SQLAlchemy ‚Äî needs CacheStore migration + in-memory fallback

**Action (Task #22)**: Unify into `CacheStoreABC` with `InMemoryCacheStore` fallback for all 4 data types.

**Future work**: Unify these into a single `CacheStoreABC` with `InMemoryCacheStore` fallback.

---

## NEXT STEPS

1. ‚úÖ Review this matrix with user
2. ‚úÖ **Step 1+2+3 COMPLETE**: All data-storage affinity decisions resolved
3. ‚ùì Identify missing Subscription/Delivery storage (Task #12)
4. ‚ùì Clarify Dragonfly status post-Raft
5. ‚úÖ Merge redundant data types (FilePathModel ‚Üí FileMetadata, WorkspaceConfig + MemoryConfig ‚Üí PathRegistrationModel)
6. ‚úÖ Rewrite federation-memo.md with this data architecture
7. ‚úÖ Storage medium orthogonality analysis complete ‚Äî Redis deprecation identified (P2)
8. ‚úÖ "Nexus Quartet" ‚Äî Four Pillars abstraction design decided (Metastore, RecordStore, ObjectStore, CacheStore)
9. ‚úÖ **COMPLETE**: Task #14 ‚Äî MetastoreABC + RecordStoreABC in NexusFS constructor (Four Pillars DI)
10. üìã **PLANNED**: Rename proto-generated `MetadataStore` ‚Üí `FileMetadataProtocol` (avoid confusion with MetastoreABC)
11. ‚úÖ **COMPLETE**: CI PyO3 build for nexus_raft (#1234)
12. ‚ùì **DECISION**: Version history (VersionHistoryGC, TimeTravelReader) ‚Äî kernel or services? (Related: Task #3, #11)
13. üÜï **PRINCIPLE**: Cross-pillar co-existence ‚Äî if a config only serves data in another pillar, it belongs in that pillar
14. ‚úÖ **Step 2 COMPLETE**: 4 orthogonal storage mediums = 4 Pillars. Redis deprecated. In-Memory merged.
15. ‚úÖ **Step 3 COMPLETE**: ReBACTuple‚ÜíRecordStore, FileEvent‚ÜíCacheStore, UserSession‚ÜíCacheStore, DirectoryEntry‚Üímerged into FileMetadata
16. ‚ö†Ô∏è **GAP**: CacheStoreABC needs InMemory impls (EventBus, PermissionCache, TigerCache) for kernel-only/dev mode

---

**END OF DATA-STORAGE-MATRIX.MD**
